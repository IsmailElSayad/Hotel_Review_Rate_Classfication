{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hotel_review_SOTA.ipynb","provenance":[{"file_id":"1qbryiW7Z8EtCEbdNB2qq6z3jUHpVdhfT","timestamp":1607505670352}],"collapsed_sections":[],"authorship_tag":"ABX9TyOdVE8vmeHGKN+Y4JxGnXlt"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eu7Pv545n9Cl","executionInfo":{"status":"ok","timestamp":1607523759788,"user_tz":-120,"elapsed":95813,"user":{"displayName":"Bilal H","photoUrl":"","userId":"17387886967102525476"}},"outputId":"809b1575-5498-40fa-b117-23c98452cd2f"},"source":["from numpy import loadtxt\n","import pandas as pd\n","\n","# to feed and train using the dev csv\n","data_train = pd.read_csv('sentiment_dataset_dev.csv')\n","# to feed and train using the train csv\n","# data_train = pd.read_csv('sentiment_dataset_train.csv')\n","\n","data_test = pd.read_csv('sentiment_dataset_test.csv')\n","# dataset.head()\n","dataset = data_train.append(data_test)\n","# print(len(data_train))\n","# print(len(data_test))\n","# print(len(dataset))\n","# print(data_train)\n","# print(data_test)\n","# print(dataset)\n","\n","# select only relevant columns\n","#dataset = dataset[[\"review\", \"rating\"]]\n","\n","# create doc2vec vector columns\n","from gensim.test.utils import common_texts\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","\n","documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(dataset[\"review\"].apply(lambda x: x.split(\" \")))]\n","\n","# train a Doc2Vec model with our text data\n","model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n","\n","# transform each document into a vector data\n","doc2vec_df = dataset[\"review\"].apply(lambda x: model.infer_vector(x.split(\" \"))).apply(pd.Series)\n","doc2vec_df.columns = [\"doc2vec_vector_\" + str(x) for x in doc2vec_df.columns]\n","dataset = pd.concat([dataset, doc2vec_df], axis=1)\n","\n","\n","# add tf-idfs columns\n","# TF computes the classic number of times the word appears in the text\n","# IDF computes the relative importance of this word which depends on how many texts the word can be found\n","# We add TF-IDF columns for every word that appear in at least 10 different texts to filter some of them and reduce the size of the final output.\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf = TfidfVectorizer(min_df = 10)\n","tfidf_result = tfidf.fit_transform(dataset[\"review\"]).toarray()\n","tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())\n","tfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\n","tfidf_df.index = dataset.index\n","dataset = pd.concat([dataset, tfidf_df], axis=1)\n","\n","# feature selection\n","label = \"rating\"\n","ignore_cols = [label, \"id\", \"review\"]\n","features = [c for c in dataset.columns if c not in ignore_cols]\n","\n","# split the data into train and test for test accuracy\n","from sklearn.ensemble import RandomForestClassifier\n","# from sklearn.model_selection import train_test_split\n","# X_train, X_test, y_train, y_test = train_test_split(dataset[features], dataset[label], test_size = 0.20, random_state = 1)  # 70% training and 30% test\n","X_train, X_test = dataset[features][:len(data_train)], dataset[features][len(data_train):]\n","y_train = dataset[label][:len(data_train)]\n","\n","\n","\n","# classification model we are going to use is the logistic regression\n","from sklearn.linear_model import LogisticRegression\n","# Create LogisticRegression classifer object\n","clf = LogisticRegression(max_iter=600)\n","# Train Classifer\n","clf = clf.fit(X_train,y_train)\n","#Predict the response for test dataset\n","y_pred = clf.predict(X_test)\n","# Model Accuracy, how often is the classifier correct?\n","# print(\"Accuracy:\",clf.score(X_test, y_test))\n","\n","data_test_result = pd.concat([data_test, pd.DataFrame(y_pred, columns=['predictated_rating'])], axis=1)\n","print(\"Test Result Data with Predictated Ratings:\")\n","print(data_test_result)\n","# if we want to save data in csv\n","# df.to_csv(file_name, sep=',', encoding='utf-8')\n","\n","\n"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Test Result Data with Predictated Review:\n","        id  ... predictated_rating\n","0        0  ...                2.0\n","1        1  ...                5.0\n","2        3  ...                1.0\n","3        4  ...                2.0\n","4        6  ...                3.0\n","...    ...  ...                ...\n","6495  7494  ...                2.0\n","6496  7496  ...                3.0\n","6497  7497  ...                3.0\n","6498  7498  ...                2.0\n","6499  7499  ...                2.0\n","\n","[6500 rows x 3 columns]\n"],"name":"stdout"}]}]}